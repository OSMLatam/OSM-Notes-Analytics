# OSM Notes Analytics - Cron Configuration Example
#
# This file contains example cron jobs for automating ETL execution
# and maintenance tasks.
#
# Installation:
#   1. Copy this file: cp etc/cron.example /tmp/osm-notes-cron
#   2. Edit paths in the file to match your system
#   3. Install: crontab /tmp/osm-notes-cron
#   4. Verify: crontab -l
#
# Author: Andres Gomez (AngocA)

# Set environment variables
SHELL=/bin/bash
PATH=/usr/local/bin:/usr/bin:/bin
HOME=/home/your-username

# Load environment (adjust path as needed)
# You may need to source your shell configuration
# . ~/.bashrc

# ==========================================
# ETL AUTOMATION
# ==========================================

# Run incremental ETL every 15 minutes
# This processes only new notes/comments since last run
*/15 * * * * /path/to/OSM-Notes-Analytics/bin/dwh/cron_etl.sh

# Alternative: Run ETL every hour (if 15 min is too frequent)
# 0 * * * * /path/to/OSM-Notes-Analytics/bin/dwh/cron_etl.sh

# ==========================================
# EXPORT TASKS
# ==========================================

# Monthly: Export CSV files and push to GitHub (1st day of month at 3 AM)
# Exports closed notes by country to CSV for AI context
# 0 3 1 * * /path/to/OSM-Notes-Analytics/bin/dwh/exportAndPushCSVToGitHub.sh >> /var/log/osm-csv-export.log 2>&1

# Daily: Export JSON files and push to GitHub (after datamarts update)
# Exports datamarts to JSON for web viewer consumption
# 45 2 * * * /path/to/OSM-Notes-Analytics/bin/dwh/exportAndPushJSONToGitHub.sh >> /var/log/osm-json-export.log 2>&1

# ==========================================
# MAINTENANCE TASKS
# ==========================================

# Weekly: VACUUM ANALYZE (Sundays at 3 AM)
# Optimizes database performance
0 3 * * 0 psql -U postgres -d osm_notes -c "VACUUM ANALYZE dwh.facts"

# Weekly: Clean old log files (Sundays at 3:30 AM)
# Removes logs older than 7 days to prevent disk fill
30 3 * * 0 find /tmp/ETL_* -type d -mtime +7 -exec rm -rf {} \; 2>/dev/null || true

# Daily: Check for old logs (daily at 4 AM)
# Reports if log directory is getting large
0 4 * * * du -sh /tmp/ETL_* 2>/dev/null | sort -h | tail -5

# ==========================================
# BACKUP TASKS (Optional)
# ==========================================

# Daily: Backup DWH schema (1 AM)
# Creates a backup of the entire dwh schema
# 0 1 * * * pg_dump -U postgres -d osm_notes -n dwh > /backups/dwh_$(date +\%Y\%m\%d).sql

# Daily: Rotate old backups (2 AM)
# Keeps only last 30 days of backups
# 0 2 * * * find /backups/dwh_*.sql -mtime +30 -delete

# ==========================================
# MACHINE LEARNING TASKS
# ==========================================

# Monthly: Intelligent ML training/retraining (1st day of month at 2 AM)
# Script automatically detects system state and decides what to do:
# - No data → do nothing (exits silently)
# - Facts + dimensions ready → initial training (basic features)
# - Datamarts populated → full training (all features)
# - Models exist → retraining (if 10%+ new data or 30+ days old)
# Takes 30-120 minutes if training is needed
# Logs go to /tmp/ml_retrain_*.log (or use ML_LOG_FILE env var to customize)
# 0 2 1 * * /path/to/OSM-Notes-Analytics/bin/dwh/ml_retrain.sh >> /tmp/ml-retrain.log 2>&1

# Alternative: Quarterly retraining (1st day of quarter at 2 AM)
# 0 2 1 */3 * /path/to/OSM-Notes-Analytics/bin/dwh/ml_retrain.sh >> /tmp/ml-retrain.log 2>&1

# Note: If you need /var/log, create the file first with sudo:
# sudo touch /var/log/ml-retrain.log
# sudo chmod 666 /var/log/ml-retrain.log
# Then use: >> /var/log/ml-retrain.log 2>&1

# ==========================================
# MONITORING TASKS (Optional)
# ==========================================

# Daily: Run ETL monitor and send report (6 AM)
# 0 6 * * * /path/to/OSM-Notes-Analytics/bin/dwh/monitor_etl.sh > /tmp/etl_daily_report.txt

# Weekly: Generate weekly summary (Mondays at 8 AM)
# 0 8 * * 1 psql -U postgres -d osm_notes -c "SELECT COUNT(*) as total_facts FROM dwh.facts" >> /tmp/weekly_summary.txt

# ==========================================
# NOTES
# ==========================================
#
# 1. Lock Files: ETL.sh automatically creates lock files to prevent concurrent execution
#    If a job takes >15 minutes, the next one will skip automatically
#
# 2. Log Files: All ETL logs are stored in /tmp/ETL_XXXXXX/ directories
#    Keep an eye on disk space and clean up regularly
#
# 3. Email Alerts: Configure mail in cron_etl.sh to receive failure notifications
#
# 4. Database Performance: Monitor query times and adjust VACUUM frequency if needed
#
# 5. Testing: Test cron jobs manually before enabling:
#    */15 * * * * echo "Test at $(date)" >> /tmp/cron_test.log
#
# ==========================================
